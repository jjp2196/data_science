{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [],
   "source": [
    "# Import Information\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train = pd.read_csv('/Users/jamespecore/Documents/Github/data_science/Projects/store_item_demand_forecasting/data/train.csv')\n",
    "test = pd.read_csv('/Users/jamespecore/Documents/Github/data_science/Projects/store_item_demand_forecasting/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data):\n",
    "    # Convert the 'date' column to datetime format\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    return data\n",
    "\n",
    "# Apply preprocessing to train and test data\n",
    "train = preprocess_data(train)\n",
    "test = preprocess_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_datetime(data):\n",
    "    # Extract additional date-related features\n",
    "    data['year'] = data['date'].dt.year\n",
    "    data['month'] = data['date'].dt.month\n",
    "    data['day'] = data['date'].dt.day\n",
    "    data['day_of_week'] = data['date'].dt.dayofweek\n",
    "    return data\n",
    "\n",
    "train = format_datetime(train)\n",
    "test = format_datetime(test)\n",
    "\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min and Max Date Retrieval Functions\n",
    "def min_date(data):\n",
    "    return data['date'].min().date()\n",
    "\n",
    "def max_date(data):\n",
    "    return data['date'].max().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniMax Date from Train Set\n",
    "print('Min date from train set:', min_date(train))\n",
    "print('Max date from train set:', max_date(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MiniMax Date from Test Set\n",
    "print('Min date from test set:', min_date(test))\n",
    "print('Max date from test set:', max_date(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the forecast lag size, which represents the number of days between the last date in the train set\n",
    "# and the last date in the test set\n",
    "lag_size = (test['date'].max().date() - train['date'].max().date()).days\n",
    "\n",
    "# Print the results\n",
    "print('Last date in the train set: %s' % train['date'].max().date())\n",
    "print('Last date in the test set: %s' % test['date'].max().date())\n",
    "print('Forecast lag size (days between train and test set):', lag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate daily sales\n",
    "daily_sales = train.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Aggregate daily sales per store\n",
    "store_daily_sales = train.groupby(['store', 'date'])['sales'].sum().reset_index()\n",
    "\n",
    "# Aggregate daily sales per item\n",
    "item_daily_sales = train.groupby(['item', 'date'])['sales'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot daily sales using Matplotlib\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_sales['date'], daily_sales['sales'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Daily Sales')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Rearrange Dataset\n",
    "def rearrange_dataset(data):\n",
    "    # Group by item, store, and date, calculate mean sales\n",
    "    data_grouped = data.sort_values('date').groupby(['item', 'store', 'date'], as_index=False)\n",
    "    data_grouped = data_grouped.agg({'sales': 'mean'})\n",
    "    data_grouped.columns = ['item', 'store', 'date', 'sales']\n",
    "    return data_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Transform Data into a Time Series Problem\n",
    "def series_to_supervised(data, window=1, lag=1, dropnan=True):\n",
    "    cols, names = list(), list()\n",
    "    for i in range(window, 0, -1):\n",
    "        cols.append(data.shift(i))\n",
    "        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n",
    "    cols.append(data)\n",
    "    names += [('%s(t)' % (col)) for col in data.columns]\n",
    "    cols.append(data.shift(-lag))\n",
    "    names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove Rows with Different Item or Store Values\n",
    "def filter_inconsistent_data(data):\n",
    "    last_item = 'item(t-%d)' % window\n",
    "    last_store = 'store(t-%d)' % window\n",
    "    data_filtered = data[(data['store(t)'] == data[last_store])]\n",
    "    data_filtered.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Reset indices of the original data DataFrame\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Remove Unwanted Columns\n",
    "def drop_unwanted_columns(data, window, lag):\n",
    "    columns_to_drop = [('%s(t+%d)' % (col, lag)) for col in ['item', 'store']]\n",
    "    for i in range(window, 0, -1):\n",
    "        columns_to_drop += [('%s(t-%d)' % (col, i)) for col in ['item', 'store']]\n",
    "    data.drop(columns_to_drop, axis=1, inplace=True)\n",
    "    data.drop(['item(t)', 'store(t)'], axis=1, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train/Validation Split\n",
    "def split_train_valid(data, labels_col, test_size=0.4, random_state=0):\n",
    "    labels = data[labels_col]\n",
    "    data = data.drop(labels_col, axis=1)\n",
    "    X_train, X_valid, Y_train, Y_valid = train_test_split(data, labels.values, test_size=test_size, random_state=random_state)\n",
    "    print('Train set shape:', X_train.shape)\n",
    "    print('Validation set shape:', X_valid.shape)\n",
    "    return X_train, X_valid, Y_train, Y_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing steps\n",
    "train_gp = rearrange_dataset(train)\n",
    "window = 29\n",
    "lag = lag_size\n",
    "series = series_to_supervised(train_gp.drop('date', axis=1), window=window, lag=lag)\n",
    "series = filter_inconsistent_data(series)\n",
    "series = drop_unwanted_columns(series, window, lag)\n",
    "X_train, X_valid, Y_train, Y_valid = split_train_valid(series, 'sales(t+%d)' % lag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the input data\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_valid shape:\", X_valid.shape)\n",
    "\n",
    "# Reshape input data for LSTM model (add a third dimension for the single channel)\n",
    "X_train_lstm = X_train.values.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_valid_lstm = X_valid.values.reshape(X_valid.shape[0], X_valid.shape[1], 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model architecture\n",
    "lstm_model = Sequential([\n",
    "    LSTM(10, activation='relu', input_shape=(X_train_lstm.shape[1], 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile LSTM model\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train LSTM model\n",
    "# history = lstm_model.fit(X_train_lstm, Y_train, epochs=10, batch_size=32, validation_data=(X_valid_lstm, Y_valid), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this exploration of time series problem-solving approaches, we've delved into various methods and their distinctions. While the focus here wasn't on achieving peak performance, you're encouraged to experiment with different hyperparameters, particularly the window size and network topology, to enhance results. If you do, please share your findings with me.\n",
    "\n",
    "I hope this journey has imparted valuable insights. Your feedback is invaluable, so feel free to share your thoughts. If you enjoyed this content, consider exploring this reference dataset to try this out yourself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
